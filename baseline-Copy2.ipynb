{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (Flatten, Dense, Input, \n",
    "                                     Dropout, BatchNormalization,\n",
    "                                     Conv2D, MaxPooling2D, \n",
    "                                     GlobalMaxPooling2D, GlobalAveragePooling2D,)\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_show(img, name='image'):\n",
    "    cv2.imshow(name, img)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(X_train,X_test):\n",
    "        mean = np.mean(X_train,axis=(0,1,2,3))\n",
    "        std = np.std(X_train, axis=(0, 1, 2, 3))\n",
    "        X_train = (X_train-mean)/(std+1e-7)\n",
    "        X_test = (X_test-mean)/(std+1e-7)\n",
    "        return X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding(img):\n",
    "    out_w = 128\n",
    "    out_h = 128\n",
    "    top_size = int((128 - img.shape[0]) / 2)\n",
    "    bottom_size = int((128 - img.shape[0]) / 2)\n",
    "    dif_1 = 128 - (top_size + bottom_size + img.shape[0])\n",
    "    top_size += dif_1\n",
    "    left_size = int((128 - img.shape[1]) / 2)\n",
    "    right_size = int((128 - img.shape[1]) / 2)\n",
    "    dif_2 = 128 - (left_size + right_size + img.shape[1])\n",
    "    left_size += dif_2    \n",
    "    if img.shape[1] <= 128:\n",
    "        constant = cv2.copyMakeBorder(img, \n",
    "                                  top_size, bottom_size, left_size, right_size, \n",
    "                                  borderType=cv2.BORDER_CONSTANT, \n",
    "                                  value=(255, 255, 255))\n",
    "    else:\n",
    "        constant = cv2.copyMakeBorder(img, \n",
    "                          top_size, bottom_size, 0, 0, \n",
    "                          borderType=cv2.BORDER_CONSTANT, \n",
    "                          value=(255, 255, 255))\n",
    "        constant = cv2.resize(constant, (128, 128))\n",
    "        \n",
    " \n",
    "    return constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_img(img):\n",
    "\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    ret, thresh1 = cv2.threshold(gray, 110, 255, cv2.THRESH_OTSU)   \n",
    "    gaussian = cv2.GaussianBlur(thresh1, (3, 3), 1)\n",
    "    kernel = np.zeros((3, 3), np.uint8)\n",
    "    erosion_1 = cv2.erode(gaussian, kernel, iterations=1)\n",
    "\n",
    "    img2 = 255 - erosion_1\n",
    "    img2 = cv2.resize(img2, (64, 64))\n",
    "    img2 = cv2.cvtColor(img2, cv2.COLOR_GRAY2BGR)\n",
    "    \n",
    "    \n",
    "    return img2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    \n",
    "    fold_list = os.listdir(train_dir)\n",
    "    trainX_list = []\n",
    "    trainY_list = []\n",
    "    testX_list = []\n",
    "    testY_list = []\n",
    "\n",
    "    for f in fold_list:\n",
    "        label = int(f)\n",
    "        img_list = os.listdir(f'{train_dir}{f}')\n",
    "        tmp_datasetX = []\n",
    "        tmp_datasetY = []\n",
    "        for img_p in img_list:\n",
    "            try:\n",
    "                img = cv2.imread(f'{train_dir}{f}/{img_p}')\n",
    "#                 if (img.shape[0] != 128) or (img.shape[1] != 128):\n",
    "#                     img = padding(img)\n",
    "                img = preprocessing_img(img)\n",
    "#                 print(img.shape)\n",
    "                label = int(f)\n",
    "                tmp_datasetX.append(img)\n",
    "                tmp_datasetY.append(label)\n",
    "            except Exception as e:\n",
    "                print(f, img_p)\n",
    "                print(e)\n",
    "        \n",
    "        trainX, testX, trainY, testY = train_test_split(tmp_datasetX, tmp_datasetY, test_size=0.2, random_state=42)\n",
    "        trainX_list.extend(trainX)\n",
    "        trainY_list.extend(trainY)\n",
    "        testX_list.extend(testX)\n",
    "        testY_list.extend(testY)\n",
    "    trainX_ary = np.array(trainX_list)\n",
    "    trainY_ary = np.array(trainY_list)\n",
    "    testX_ary = np.array(testX_list)\n",
    "    testY_ary = np.array(testY_list)\n",
    "    print(f'trainX: {trainX_ary.shape}')\n",
    "    print(f'trainY: {trainY_ary.shape}')\n",
    "    print(f'testX: {testX_ary.shape}')\n",
    "    print(f'testY: {testY_ary.shape}')\n",
    "        \n",
    "    return trainX_ary, testX_ary, trainY_ary, testY_ary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = r'./data-clean/'\n",
    "df = pd.read_csv('word_idx_df.csv', converters={'idx':str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainX: (47930, 64, 64, 3)\n",
      "trainY: (47930,)\n",
      "testX: (12391, 64, 64, 3)\n",
      "testY: (12391,)\n"
     ]
    }
   ],
   "source": [
    "trainX, testX, trainY, testY = load_data(train_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainY = keras.utils.to_categorical(trainY, num_classes=800)\n",
    "testY = keras.utils.to_categorical(testY, num_classes=800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(47930, 64, 64, 3)\n",
      "(12391, 64, 64, 3)\n",
      "(47930, 800)\n",
      "(12391, 800)\n"
     ]
    }
   ],
   "source": [
    "print(trainX.shape)\n",
    "print(testX.shape)\n",
    "print(trainY.shape)\n",
    "print(testY.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(47930, 128, 128, 3)\n",
      "(12391, 128, 128, 3)\n",
      "(47930, 800)\n",
      "(12391, 800)\n"
     ]
    }
   ],
   "source": [
    "print(trainX.shape)\n",
    "print(testX.shape)\n",
    "print(trainY.shape)\n",
    "print(testY.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8463"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor = Input(shape=(64, 64, 3))\n",
    "model = keras.applications.Xception(include_top=False, \n",
    "                                    weights='imagenet',\n",
    "                                    input_tensor=input_tensor,\n",
    "                                    pooling=None,\n",
    "                                    classes=800)\n",
    "x = model.output\n",
    "x = Flatten()(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "# x = Dense(1024, activation='relu')(x)\n",
    "# x = BatchNormalization()(x)\n",
    "predictions = Dense(800, activation='softmax')(x)\n",
    "model = Model(inputs=model.input, outputs=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "136"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.layers[:66]:\n",
    "    layer.trainable = False\n",
    "for layer in model.layers[66:]:\n",
    "    layer.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'block8_sepconv1_act'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[66].name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras import optimizers, callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = optimizers.Adam(lr=0.001) \n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=10, verbose=0),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=7, verbose=1, min_delta=1e-4, mode='min')\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1498/1498 - 63s - loss: 6.2648 - accuracy: 0.0171 - val_loss: 5.4186 - val_accuracy: 0.0392 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "1498/1498 - 63s - loss: 4.2624 - accuracy: 0.1235 - val_loss: 3.8140 - val_accuracy: 0.1763 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "1498/1498 - 64s - loss: 3.0933 - accuracy: 0.2834 - val_loss: 3.0437 - val_accuracy: 0.3036 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "1498/1498 - 64s - loss: 2.3669 - accuracy: 0.4144 - val_loss: 2.5177 - val_accuracy: 0.3980 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "1498/1498 - 64s - loss: 1.8513 - accuracy: 0.5215 - val_loss: 2.3458 - val_accuracy: 0.4513 - lr: 0.0010\n",
      "Epoch 6/50\n",
      "1498/1498 - 64s - loss: 1.4758 - accuracy: 0.6051 - val_loss: 2.2139 - val_accuracy: 0.4860 - lr: 0.0010\n",
      "Epoch 7/50\n",
      "1498/1498 - 64s - loss: 1.1560 - accuracy: 0.6771 - val_loss: 2.2985 - val_accuracy: 0.4837 - lr: 0.0010\n",
      "Epoch 8/50\n",
      "1498/1498 - 64s - loss: 0.8948 - accuracy: 0.7436 - val_loss: 2.1825 - val_accuracy: 0.5297 - lr: 0.0010\n",
      "Epoch 9/50\n",
      "1498/1498 - 64s - loss: 0.7004 - accuracy: 0.7921 - val_loss: 2.2396 - val_accuracy: 0.5296 - lr: 0.0010\n",
      "Epoch 10/50\n",
      "1498/1498 - 64s - loss: 0.5490 - accuracy: 0.8324 - val_loss: 2.3981 - val_accuracy: 0.5274 - lr: 0.0010\n",
      "Epoch 11/50\n",
      "1498/1498 - 65s - loss: 0.4362 - accuracy: 0.8647 - val_loss: 2.3966 - val_accuracy: 0.5439 - lr: 0.0010\n",
      "Epoch 12/50\n",
      "1498/1498 - 65s - loss: 0.3973 - accuracy: 0.8747 - val_loss: 2.4596 - val_accuracy: 0.5464 - lr: 0.0010\n",
      "Epoch 13/50\n",
      "1498/1498 - 65s - loss: 0.3247 - accuracy: 0.8959 - val_loss: 2.7573 - val_accuracy: 0.5215 - lr: 0.0010\n",
      "Epoch 14/50\n",
      "1498/1498 - 65s - loss: 0.2816 - accuracy: 0.9099 - val_loss: 2.6370 - val_accuracy: 0.5459 - lr: 0.0010\n",
      "Epoch 15/50\n",
      "\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "1498/1498 - 65s - loss: 0.2637 - accuracy: 0.9155 - val_loss: 2.7328 - val_accuracy: 0.5468 - lr: 0.0010\n",
      "Epoch 16/50\n",
      "1498/1498 - 65s - loss: 0.1045 - accuracy: 0.9671 - val_loss: 2.1220 - val_accuracy: 0.6205 - lr: 1.0000e-04\n",
      "Epoch 17/50\n",
      "1498/1498 - 65s - loss: 0.0447 - accuracy: 0.9878 - val_loss: 2.0932 - val_accuracy: 0.6297 - lr: 1.0000e-04\n",
      "Epoch 18/50\n",
      "1498/1498 - 65s - loss: 0.0283 - accuracy: 0.9939 - val_loss: 2.0873 - val_accuracy: 0.6304 - lr: 1.0000e-04\n",
      "Epoch 19/50\n",
      "1498/1498 - 65s - loss: 0.0215 - accuracy: 0.9956 - val_loss: 2.0830 - val_accuracy: 0.6347 - lr: 1.0000e-04\n",
      "Epoch 20/50\n",
      "1498/1498 - 65s - loss: 0.0163 - accuracy: 0.9969 - val_loss: 2.0886 - val_accuracy: 0.6370 - lr: 1.0000e-04\n",
      "Epoch 21/50\n",
      "1498/1498 - 65s - loss: 0.0131 - accuracy: 0.9979 - val_loss: 2.1025 - val_accuracy: 0.6409 - lr: 1.0000e-04\n",
      "Epoch 22/50\n",
      "1498/1498 - 65s - loss: 0.0112 - accuracy: 0.9982 - val_loss: 2.0985 - val_accuracy: 0.6430 - lr: 1.0000e-04\n",
      "Epoch 23/50\n",
      "1498/1498 - 65s - loss: 0.0102 - accuracy: 0.9983 - val_loss: 2.1154 - val_accuracy: 0.6439 - lr: 1.0000e-04\n",
      "Epoch 24/50\n",
      "1498/1498 - 65s - loss: 0.0084 - accuracy: 0.9988 - val_loss: 2.1417 - val_accuracy: 0.6430 - lr: 1.0000e-04\n",
      "Epoch 25/50\n",
      "1498/1498 - 65s - loss: 0.0066 - accuracy: 0.9992 - val_loss: 2.1364 - val_accuracy: 0.6467 - lr: 1.0000e-04\n",
      "Epoch 26/50\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "1498/1498 - 65s - loss: 0.0065 - accuracy: 0.9991 - val_loss: 2.1689 - val_accuracy: 0.6422 - lr: 1.0000e-04\n",
      "Epoch 27/50\n",
      "1498/1498 - 65s - loss: 0.0053 - accuracy: 0.9993 - val_loss: 2.1461 - val_accuracy: 0.6439 - lr: 1.0000e-05\n",
      "Epoch 28/50\n",
      "1498/1498 - 65s - loss: 0.0046 - accuracy: 0.9996 - val_loss: 2.1399 - val_accuracy: 0.6442 - lr: 1.0000e-05\n",
      "Epoch 29/50\n",
      "1498/1498 - 65s - loss: 0.0041 - accuracy: 0.9996 - val_loss: 2.1378 - val_accuracy: 0.6459 - lr: 1.0000e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x16f4b24f088>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(trainX, trainY, batch_size=32, epochs=50, verbose=2,\n",
    "          validation_data=(testX, testY), callbacks=callbacks ) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv-gpu",
   "language": "python",
   "name": "cv-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
